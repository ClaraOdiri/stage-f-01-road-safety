{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --user --upgrade pip\n",
    "#!pip3 install pandas==0.23.4 matplotlib==3.0.3 scipy==1.2.1 scikit-learn==0.23.1 tensorflow==2.0 keras==1.2.2 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install kubeflow pipeline sdk\n",
    "#!pip3 install kfp --user\n",
    "#!pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for pipeline\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create  directory for outputs.\n",
    "output_dir = \"/home/jovyan/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessing fucntion\n",
    "\n",
    "def preprocess(data_path):\n",
    "    \n",
    "    # Import Libraries\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # import data\n",
    "    \n",
    "    file_1= pd.read_csv('https://raw.githubusercontent.com/HamoyeHQ/07-road-safety/master/data/Cas.csv')\n",
    "    file_2= pd.read_csv('https://raw.githubusercontent.com/HamoyeHQ/07-road-safety/master/data/dftRoadSafety_Accidents_2016.csv')\n",
    "    file_3= pd.read_csv(\"https://raw.githubusercontent.com/HamoyeHQ/07-road-safety/master/data/MakeModel2016.csv\")\n",
    "\n",
    "    pre_data= file_1.merge(file_2, on=\"Accident_Index\",how=\"inner\")\n",
    "    dataset= pre_data.merge(file_3, on=\"Accident_Index\", how=\"inner\")\n",
    "    \n",
    "    # dropping the Accident_Index column\n",
    "    dataset.drop(columns='Accident_Index', axis=1, inplace=True)\n",
    "    \n",
    "    pd.set_option('display.expand_frame_repr', False) \n",
    "    dataset.mask(dataset==-1, inplace=True)\n",
    "    \n",
    "    # dropping the NaN rows\n",
    "    dataset.dropna(how='any',inplace=True)\n",
    "    \n",
    "    # dropping correlated and unnecessary columns for prediction\n",
    "    dataset.drop(columns=['make','model','accyr','Longitude','Latitude',\"Age_of_Casualty\",'Pedestrian_Location','Casualty_Severity','Police_Force','Casualty_Reference', \\\n",
    "                      'Junction_Control','2nd_Road_Class', 'Local_Authority_(Highway)', 'LSOA_of_Accident_Location','1st_Road_Class','1st_Road_Number','2nd_Road_Number'], axis=1, inplace=True)\n",
    "    # function for obtaining month in date column\n",
    "    \n",
    "    # Covert 'Date' to proper datetime format\n",
    "    dataset['Date']= pd.to_datetime(dataset['Date'])\n",
    "    \n",
    "    # extract month from date\n",
    "    dataset['Month'] = pd.DatetimeIndex(dataset['Date']).month\n",
    "    \n",
    "    # extract hour from time\n",
    "    dataset['Hour_of_the_day'] = pd.to_datetime(dataset['Time']).dt.hour\n",
    "    \n",
    "    #  dropping time and date as they are no longer useful\n",
    "    dataset.drop(columns=['Time', 'Date'],axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # serialize clean data to output directory\n",
    "    with open(f'{data_path}/clean_data','wb') as f:\n",
    "        pickle.dump((dataset),f)\n",
    "        \n",
    "    \n",
    "    return (print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and prediction function\n",
    "\n",
    "def train_predict(data_path):\n",
    "    \n",
    "    # import Library\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','xgboost'])\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    #import imblearn\n",
    "    #from imblearn.over_sampling import SMOTE\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import  f1_score, accuracy_score\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    # deserialize clean data from output directory\n",
    "    with open(f'{data_path}/clean_data','rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create features and targets\n",
    "    \n",
    "    X= dataset.drop(columns=['Accident_Severity'])\n",
    "    Y= dataset.Accident_Severity\n",
    "\n",
    "    # split data based on y categories\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)\n",
    "    \n",
    "    # normalize the dataset\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    normalised_train_df = scaler.fit_transform(x_train)\n",
    "    normalised_test_df = scaler.transform(x_test)\n",
    "\n",
    "    x_train = normalised_train_df\n",
    "    x_test = normalised_test_df\n",
    "    \n",
    "    # Oversampling to balance the imbalanced data\n",
    "    \n",
    "    #smote = SMOTE(random_state=1)\n",
    "    #x_train_balanced, y_balanced = smote.fit_sample(x_train, y_train)\n",
    "    \n",
    "\n",
    "    # I commented out the lines of code above cause of the attribute error it raised.\n",
    "    \n",
    "    #using xgboost classifier\n",
    "    xg_class = xgb.XGBClassifier(objective ='binary:logistic', random_state= 1)\n",
    "    xg_class.fit(x_train, y_train)\n",
    "    \n",
    "    # checking predicitions\n",
    "    new_predictions2 =xg_class.predict(x_test)\n",
    "    \n",
    "    # accuracy score\n",
    "    accuracy2 = accuracy_score(y_true=y_test, y_pred=new_predictions2)\n",
    "    print('Accuracy: {}'.format(round(accuracy2*100), 2))\n",
    "    \n",
    "    # f1 score\n",
    "    f1_xgb = f1_score(y_test, new_predictions2, average='micro')\n",
    "    print('f1 score: {:.2f}'.format(f1_xgb))\n",
    "    \n",
    "    \n",
    "    # write predictions to results.txt\n",
    "    with open(f'{data_path}/results.txt','w') as result:\n",
    "        result.write(f'Prediciton: {new_predictions2} | Actual {y_test}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create light weight components\n",
    "\n",
    "preprocess_op = comp.func_to_container_op(preprocess)\n",
    "train_predict_op = comp.func_to_container_op(train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client that would enable communication with the Pipelines API server \n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name=\"Road Safety ML Pipeline 2\", description=\"Performs Preprocessing, training and prediction\")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def road_safety_pipeline(data_path: str ):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO) #RWO\n",
    "\n",
    "    # Create preprocess components.\n",
    "    road_safety_preprocess_container = preprocess_op(data_path).add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # Create train&prediction component.\n",
    "    road_safety_train_predict_container = train_predict_op(data_path).add_pvolumes({data_path: road_safety_preprocess_container.pvolume})\n",
    "\n",
    "\n",
    "    # Print the result of the prediction\n",
    "    road_safety_result_container = dsl.ContainerOp(\n",
    "            name=\"print_prediction\",\n",
    "            image='library/bash:4.4.23', \n",
    "            pvolumes={data_path: road_safety_train_predict_container.pvolume},\n",
    "            arguments=['cat', f'{data_path}/results.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH =\"/home/jovyan/data/clean_data\"\n",
    "\n",
    "\n",
    "pipeline_func = road_safety_pipeline\n",
    "\n",
    "experiment_name = 'road_safety2_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
